{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRSYnTzlAvKJ"
   },
   "source": [
    "# Introduction to Pandas 2 | Narrow versus Wide and more\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "> This is an aggregated tutorial relying on material for the following fantastic sources:\n",
    "-  [Justin Bois](http://justinbois.github.io/bootcamp/2020/index.html). It contains modified training datasets and adopts content to Colab environment.\n",
    "- [BIOS821 course at Duke](https://people.duke.edu/~ccc14/bios-821-2017/index.html)\n",
    "- [Pandas documentation](https://pandas.pydata.org/docs/user_guide/index.html/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyxPi6FzAvKM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcTUu1FfAvKN"
   },
   "source": [
    "<hr>\n",
    "\n",
    "In the last lesson, we learned about Pandas and dipped our toe in to see its power. In this lesson, we will continue to harness the power of Pandas to pull out subsets of data we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbLylpWbAvKN"
   },
   "source": [
    "## Tidy data\n",
    "\n",
    "[Hadley Wickham](https://en.wikipedia.org/wiki/Hadley_Wickham) wrote a [great article](http://dx.doi.org/10.18637/jss.v059.i10) in favor of \"tidy data.\" Tidy data frames follow the rules:\n",
    "\n",
    "1. Each variable is a column.\n",
    "2. Each observation is a row.\n",
    "3. Each type of observation has its own separate data frame.\n",
    "\n",
    "This is less pretty to visualize as a table, but we rarely look at data in tables. Indeed, the representation of data which is convenient for visualization is different from that which is convenient for analysis. A tidy data frame is almost always **much** easier to work with than non-tidy formats.\n",
    "\n",
    "Also, let's take a look at this [article](https://dtkaplan.github.io/DataComputingEbook/chap-tidy-data.html#chap:tidy-data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfjlZ0cY5n7m"
   },
   "source": [
    "## The data set\n",
    "\n",
    "The dataset we will be using is a list of all SARS-CoV-2 datasets in [Sequence Read Archive](https://www.ncbi.nlm.nih.gov/sra) as of January 20, 2021.  \n",
    "\n",
    "It is obtained by going to https://www.ncbi.nlm.nih.gov/sra and performing a query with the following search terms: `txid2697049[Organism:noexp]`.\n",
    "\n",
    "Results are downloaded using `Send to:` menu selecting `File` and then `RunInfo`. Let's get these results into this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "7cqmUaLQAvKO",
    "outputId": "3a8a73f9-91d4-41fb-cc42-baaf4f594e23"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/nekrut/BMMB554/raw/master/2021/data/sra_ncov_bmmb554.csv.gz')\n",
    "df = df[df['size_MB']> 0].reset_index(drop=True)\n",
    "\n",
    "# Take a look\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wxzDARLAvKO"
   },
   "source": [
    "This data set is in tidy format. Each row represents a single SRA dataset. The properties of each run are given in each column. We already saw the power of having the data in this format when we did Boolean indexing in the last lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYh4hjEiCMcF"
   },
   "source": [
    "## Finding unique values and counts\n",
    "\n",
    "How many unique sequencing platforms do we have?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FTZWfg2ChEq",
    "outputId": "f220a81c-be65-42ad-ab85-bfc9f7af22a6"
   },
   "outputs": [],
   "source": [
    "df['Platform'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJcSVudSCqlw",
    "outputId": "0c28fdf0-21d4-458b-e214-b28626e6a82f"
   },
   "outputs": [],
   "source": [
    "df['Platform'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKQg1aauDFKd"
   },
   "source": [
    "## Sorting\n",
    "\n",
    "(and axes!)\n",
    "\n",
    "Let's start by sorting on index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3811ySUDKvN"
   },
   "outputs": [],
   "source": [
    "df_subset = df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "2Q20tuKyDVr-",
    "outputId": "06bf5c88-f62e-461c-8b18-e49e77b1337c"
   },
   "outputs": [],
   "source": [
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "Bsp6Chr9EEaO",
    "outputId": "84281323-fc5f-47f8-df5a-3f789a6cd223"
   },
   "outputs": [],
   "source": [
    "df_subset.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "11OlMbJQDtPU",
    "outputId": "446c0aa1-4a0e-47da-e4c1-b9cb61ba78fb"
   },
   "outputs": [],
   "source": [
    "df_subset.sort_index(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSMduI6iEUgC"
   },
   "source": [
    "Now let's try sorting by values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "sCo_SlUtD0Si",
    "outputId": "45c32cfd-7fb3-4062-c512-bca256693a6e"
   },
   "outputs": [],
   "source": [
    "df_subset.sort_values(by=['LibraryLayout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "nMFm51KPEo45",
    "outputId": "09c89312-a755-4009-eb4e-ddedab0cb9ab"
   },
   "outputs": [],
   "source": [
    "df_subset.sort_values(by=['LibraryLayout','size_MB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "VUtdMqWKEvV5",
    "outputId": "0b3ba554-a4d7-4733-b8d8-ccbe0c5f8d38"
   },
   "outputs": [],
   "source": [
    "df_subset.sort_values(by=['LibraryLayout','size_MB'],ascending=[True,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l-9ICXRAvKP"
   },
   "source": [
    "## Split-apply-combine\n",
    "\n",
    "The general idea of \"Split-Apply-Combine\" is shown  in this figure:\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/60a1e7e95eaef8f9a99f43335368915eafedda3e/687474703a2f2f7777772e686f66726f652e6e65742f737461743537392f736c696465732f73706c69742d6170706c792d636f6d62696e652e706e67\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "> Image from [BIOS703](https://people.duke.edu/~ccc14/bios-821-2017/index.html#)\n",
    "\n",
    "Let's say we want to compute the total size of SRA runs for each `BioProject`. Ignoring for the second the mechanics of how we would do this with Pandas, let's think about it in English. What do we need to do?\n",
    "\n",
    "1. **Split** the data set up according to the `'BioProject'` field, i.e., split it up so we have a separate data set for each BioProject ID. .\n",
    "2. **Apply** a median function to the activity in these split data sets.\n",
    "3. **Combine** the results of these averages on the split data set into a new, summary data set that contains classes for each platform and medians for each.\n",
    "\n",
    "We see that the strategy we want is a **split-apply-combine** strategy. This idea was put forward by Hadley Wickham in [this paper](http://dx.doi.org/10.18637/jss.v040.i01). It turns out that this is a strategy we want to use *very* often. Split the data in terms of some criterion. Apply some function to the split-up data. Combine the results into a new data frame.\n",
    "\n",
    "Note that if the data are tidy, this procedure makes a lot of sense. Choose the column you want to use to split by. All rows with like entries in the splitting column are then grouped into a new data set. You can then apply any function you want into these new data sets. You can then combine the results into a new data frame.\n",
    "\n",
    "Pandas's split-apply-combine operations are achieved using the `groupby()` method. You can think of `groupby()` as the splitting part. You can then apply functions to the resulting `DataFrameGroupBy` object. The [Pandas documentation on split-apply-combine](http://pandas.pydata.org/pandas-docs/stable/groupby.html) is excellent and worth reading through. It is extensive though, so don't let yourself get intimidated by it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA8SeUXtAvKP"
   },
   "source": [
    "### Aggregation\n",
    "\n",
    "Let's go ahead and do our first split-apply-combine on this tidy data set. First, we will split the data set up by `BioProject`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnWdwqP_AvKP",
    "outputId": "e4f977a7-649b-4fd3-ce17-20806378e358"
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby(['BioProject'])\n",
    "\n",
    "# Take a look\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDC468BYAvKP"
   },
   "source": [
    "There is not much to see in the `DataFrameGroupBy` object that resulted. But there is a lot we can do with this object. Typing `grouped.` and hitting tab will show you the many possibilities. For most of these possibilities, the apply and combine steps happen together and a new data frame is returned. The `grouped.sum()` method is exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "EK3ivLmPAvKQ",
    "outputId": "7626b651-a826-40dc-c10f-e3844653eca7"
   },
   "outputs": [],
   "source": [
    "df_sum = grouped.sum()\n",
    "\n",
    "# Take a look\n",
    "df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "DkdJDw_n-Hyc",
    "outputId": "6c54a64f-2f78-4b2f-bb29-f6c4ecf93d90"
   },
   "outputs": [],
   "source": [
    "df_sum = pd.DataFrame(grouped['size_MB'].sum())\n",
    "df_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg3kwR0QAvKQ"
   },
   "source": [
    "The outputted data frame has the sums of numerical columns only, which we have only one: `size_MS`. Note that this data frame has `Platform` as the name of the row index. If we want to instead keep `Platform` (which, remember, is what we used to split up the data set before we computed the summary statistics) as a column, we can use the `reset_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "iPk_ipqWAvKQ",
    "outputId": "38302244-4252-4b11-9402-18ca3b9e1b00"
   },
   "outputs": [],
   "source": [
    "df_sum.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSR7ZmjcAvKQ"
   },
   "source": [
    "Note, though, that this was not done in-place. If you want to update your data frame, you have to explicitly do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4MtmaNRAvKQ"
   },
   "outputs": [],
   "source": [
    "df_sum = df_sum.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia3tHWi0AvKQ"
   },
   "source": [
    "We can also use multiple columns in our `groupby()` operation. To do this, we simply pass in a list of columns into `df.groupby()`. We will **chain the methods**, performing a groupby, applying a median, and then resetting the index of the result, all in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "TkVY7-daAvKR",
    "outputId": "6e2e5b88-1a81-4ca9-b55e-c35b96497d5e"
   },
   "outputs": [],
   "source": [
    "df.groupby(['BioProject', 'Platform']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS_4gFEjAvKR"
   },
   "source": [
    "This type of operation is called an **aggregation**. That is, we split the data set up into groups, and then computed a summary statistic for each group, in this case the median. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkzWexQxAvKU"
   },
   "source": [
    "You now have tremendous power in your hands. When your data are tidy, you can rapidly accelerate the ubiquitous split-apply-combine methods. Importantly, you now have a logical framework to think about how you slice and dice your data. As a final, simple example, I will show you how to go start to finish after loading the data set into a data frame, splitting by `BioProject` and `Platform`, and then getting the quartiles and extrema, in addition to the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "TgWW6_atAvKU",
    "outputId": "44aaec88-6f0e-43a5-e90d-98ce8ac6179d"
   },
   "outputs": [],
   "source": [
    "df.groupby(['BioProject', 'Platform'])['size_MB'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "oJahohiSPuJT",
    "outputId": "eb01e621-9360-4919-a6b5-7880efbc1588"
   },
   "outputs": [],
   "source": [
    "df.groupby(['BioProject', 'Platform'])['size_MB'].describe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "oJahohiSPuJT",
    "outputId": "eb01e621-9360-4919-a6b5-7880efbc1588"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df.groupby(['BioProject', 'Platform']).agg({'size_MB':np.mean, 'Run':'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_249LZXAvKU"
   },
   "source": [
    "Yes, that's right. One single, clean, easy to read line of code. In coming tutorials, we will see how to use tidy data to quickly generate plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why `np.mean` is without quotes and `nunique` is with quotes? See [here](https://stackoverflow.com/questions/66443260/why-are-some-pandas-aggregation-functions-in-quotes-and-others-not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnGai6hXAvKV"
   },
   "source": [
    "## Tidying a data set\n",
    "\n",
    "You should always organize your data sets in a tidy format. However, this is sometimes just not possible, since you data sets can come from instruments that do not output the data in tidy format (though most do, at least in my experience), and you often have collaborators that send data in untidy formats.\n",
    "\n",
    "The most useful function for tidying data is `pd.melt()`. To demonstrate this we will use a dataset describing read coverage across SARS-CoV-2 genomes for a number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xZ-RUo_5AvKV",
    "outputId": "dbe0c483-3733-4a26-95b7-2b4e99cf3b42"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/nekrut/BMMB554/raw/master/2021/data/coverage.tsv',sep='\\t')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjf2ObpnAvKV"
   },
   "source": [
    "Clearly these data are not tidy. When we melt the data frame, the data within it, called **values**, become a single column. The headers, called **variables**, also become new columns. So, to melt it, we need to specify what we want to call the values and what we want to call the variable. [`pd.melt()`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html#pandas.melt) does the rest!\n",
    "\n",
    "![](https://pandas.pydata.org/docs/_images/07_melt.svg)\n",
    "\n",
    "> Image from [Pandas Docs](https://pandas.pydata.org/docs/getting_started/intro_tutorials/07_reshape_table_layout.html#wide-to-long-format).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "BK2HAA9VAvKV",
    "outputId": "8b599ba2-9375-444a-db76-c31d5d9d8ad0"
   },
   "outputs": [],
   "source": [
    "melted = pd.melt(df, value_name='coverage', var_name=['sample'],value_vars=df.columns[3:],id_vars=['start','end'])\n",
    "\n",
    "melted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHUsi9a4iQl6"
   },
   "source": [
    "...and we are good to do with a tidy DataFrame! Let's take a look at the summary. This wouild allow us to easily plot coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "rnIfWRPNhoaZ",
    "outputId": "c5b8e36f-fc5d-463b-9757-e06b4abf15b8"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.relplot(data=melted, x='start',y='coverage',kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "Rsc81-xUiW3L",
    "outputId": "34c55264-a470-4274-be2b-1caaad8d519f"
   },
   "outputs": [],
   "source": [
    "sns.relplot(data=melted, x='start',y='coverage',kind='line',hue='sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "HMvbRqcVAvKV",
    "outputId": "a49ff12a-4a56-4fef-8f93-10e8d9e6d30b"
   },
   "outputs": [],
   "source": [
    "melted.groupby(['sample']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "bvNC-c-BTkOX",
    "outputId": "a7d917e5-37b0-4440-c275-a2fa1c1f82c5"
   },
   "outputs": [],
   "source": [
    "melted.groupby(['sample'])['coverage'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbRHzxTCZhRe"
   },
   "source": [
    "To get back from melted (narrow) format to wide format we can use [`pivot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html#pandas.DataFrame.pivot) function. \n",
    "\n",
    "![](https://pandas.pydata.org/docs/_images/07_pivot.svg)\n",
    "\n",
    "> Image from [Pandas Docs](https://pandas.pydata.org/docs/getting_started/intro_tutorials/07_reshape_table_layout.html#long-to-wide-table-format).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Ta7-RogiT9n_",
    "outputId": "5b4d9cc9-423f-48e5-8db6-e735f7ddf84c"
   },
   "outputs": [],
   "source": [
    "melted.pivot(index=['start','end'],columns='sample',values='coverage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jy-eJFwgZP9F"
   },
   "source": [
    "## Working with multiple tables\n",
    "\n",
    "Working with multiple tables often involves joining them on a common key:\n",
    "\n",
    "![](https://pandas.pydata.org/docs/_images/08_merge_left.svg)\n",
    "\n",
    "In fact, this can be done in several different ways described below. But first let's define two simple dataframes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6hN0L6actsX",
    "outputId": "011a0a45-06bb-4d86-e7eb-1c9ed14cd1ab"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMB0BReUc1EU"
   },
   "outputs": [],
   "source": [
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URPgwUbNcQGg"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"key\": [\"A\", \"B\", \"C\", \"D\"], \"value\": np.random.randn(4)})\n",
    "df2 = pd.DataFrame({\"key\": [\"B\", \"D\", \"D\", \"E\"], \"value\": np.random.randn(4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "dIjlTLvx5YeA",
    "outputId": "c95d1428-29b8-40f2-f76c-7f2eee87b75e"
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "6QM0BQeT5a6a",
    "outputId": "f38ec76f-6817-43a6-be5d-ae9b00729d3d"
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W7qRHyzcmNl"
   },
   "source": [
    "### Inner join\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/SQL_Join_-_07_A_Inner_Join_B.svg/234px-SQL_Join_-_07_A_Inner_Join_B.svg.png?20170204165143)\n",
    "\n",
    "> Figure from Wikimedia Commons\n",
    "\n",
    "Using pandas `merge`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "oPjL9gzwcUSk",
    "outputId": "6ab82c7f-15b5-40aa-9053-a6bc09dccfd3"
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRQd_lXVc7py"
   },
   "source": [
    "Using `pysqldf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "rtgKktMmc_ce",
    "outputId": "d90df0c0-7a88-4b65-afbd-d8e1cda249c6"
   },
   "outputs": [],
   "source": [
    "pysqldf('select * from df1 join df2 on df1.key=df2.key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KMMLInKdK5m"
   },
   "source": [
    "### Left join\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/dc/SQL_Join_-_01b_A_Left_Join_B.svg/234px-SQL_Join_-_01b_A_Left_Join_B.svg.png?20170204144906)\n",
    "\n",
    "> Figure from Wikimedia Commons\n",
    "\n",
    "Using pandas `merge`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tNv-5iQ4dKhr",
    "outputId": "b1ba0307-66f5-4f90-ff95-fdef113b78a2"
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=\"key\", how=\"left\").fillna('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnDN6_m-dcGu"
   },
   "source": [
    "Using `pysqldf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "LJXeglUPdc7v",
    "outputId": "0de8b8a5-15f7-485c-c373-bca94c30bc64"
   },
   "outputs": [],
   "source": [
    "pysqldf('select * from df1 left join df2 on df1.key=df2.key').fillna('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "m7AxUDBsd05i",
    "outputId": "91f67205-2fc7-4a52-dce2-179fa6339751"
   },
   "outputs": [],
   "source": [
    "pysqldf('select df1.key, df1.value as value_x, df2.value as value_y from df1 left join df2 on df1.key=df2.key').fillna('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "022ivKMhd-Ja"
   },
   "source": [
    "### Right join\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/SQL_Join_-_03_A_Right_Join_B.svg/234px-SQL_Join_-_03_A_Right_Join_B.svg.png?20170130230641)\n",
    "\n",
    "> Figure from Wikimedia Commons\n",
    "\n",
    "Using pandas `merge`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "oeiQ7SFNeGS9",
    "outputId": "55a41745-038a-46cc-ecf9-fd20838c9ae2"
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=\"key\", how=\"right\").fillna('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roB02uhkejT5"
   },
   "source": [
    "### Full join\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/61/SQL_Join_-_05_A_Full_Join_B.svg/234px-SQL_Join_-_05_A_Full_Join_B.svg.png?20170130230643)\n",
    "\n",
    "> Figure from Wikimedia Commons\n",
    "\n",
    "Using pandas `merge`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "sk6FS9TTerCT",
    "outputId": "10e609e6-dee3-4919-bc12-e15ab9491b3a"
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=\"key\", how=\"outer\").fillna('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrr9RPNqAvKW"
   },
   "source": [
    "## Computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-FVBnrGFWFh"
   },
   "outputs": [],
   "source": [
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BFmDQDtAvKW"
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "Pandas2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
